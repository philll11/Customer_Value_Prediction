{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Load libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets.\n",
    "path = '../input/santander-value-prediction-challenge/'\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop([\"ID\", \"target\"], axis=1)\n",
    "y = np.log1p(train[\"target\"].values)\n",
    "\n",
    "test = test.drop([\"ID\"], axis=1)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "First we want to get an idea of what we are working with. Lets have a look at the shape, head, tail and the data types in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The train dataset has {X.shape[0]} rows and {X.shape[1]} columns')\n",
    "print(f'The test dataset has {test.shape[0]} rows and {test.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtype = X.dtypes.unique().tolist()\n",
    "test_dtype = test.dtypes.unique().tolist()\n",
    "\n",
    "print(f'The train dataset contains {train_dtype[0]}, {train_dtype[1]} and {train_dtype[2]} data types')\n",
    "print(f'The test dataset contains {test_dtype[0]} and {test_dtype[1]} data types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only the ID column is of type 'object', we have no categorical variables to encoded.\n",
    "\n",
    "Just from the initial look, it seems we have a very sparse dataset for both the train and test data. All columns have been anonymised so it will be tricky to get an intuition on what variables we want to use. We also see that there are more features than rows so manual feature selection will not be feasible. \n",
    "The target variable seems to consist of very large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_na = y.isna().any().sum()\n",
    "X_na = X.isna().any().sum()\n",
    "test_na = test.isna().any().sum()\n",
    "print(f'{y_na} contain missing values in the target data')\n",
    "print(f'{X_na} out of {len(X.columns)} columns contain missing values in the predictor data')\n",
    "print(f'{test_na} out of {len(test.columns)} columns contain missing values in the test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can conclude there is not missing values meaning not data imputation will be needed.\n",
    "\n",
    "Next, we will explore the features more in-depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Target minimum & maximum values: {np.min(y)} & {np.max(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'TransactionAmt' feature\n",
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 3))\n",
    "\n",
    "# Not Fraud\n",
    "ax1.title.set_text('Tagret distribution')\n",
    "ax1.set_xlabel('Target')\n",
    "ax1.set_ylabel('Number of occuraces')\n",
    "_ = ax1.hist(y, bins=100)\n",
    "\n",
    "# Fraud\n",
    "ax2.title.set_text('log Tagret distribution')\n",
    "ax2.set_xlabel('Target')\n",
    "ax2.set_ylabel('Number of occuraces')\n",
    "_ = ax2.hist(np.log(y), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a log transformation, we see a flat distribution occuring with larget peaks between 12 and 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Before we do any feature selection, all constant values should be removed as they offer nothing to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes features containing constant values\n",
    "feat_to_remove = []\n",
    "for feat in X.columns:\n",
    "    if len(X[feat].unique()) == 1:\n",
    "        feat_to_remove.append(feat)\n",
    "        \n",
    "X.drop(feat_to_remove, axis=1, inplace=True)\n",
    "test.drop(feat_to_remove, axis=1, inplace=True)\n",
    "\n",
    "print(f'Removed {len(feat_to_remove)} Constant Columns\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Features\n",
    "As our training data is anonymised, we must use statistical methodlegy to examine which features might produce good results.\n",
    "\n",
    "We will start by producing a correlation plot with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = X.drop('ID', axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = corr_df.where((corr_df != 1) & (corr_df > 0.75) | (corr_df < -0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {}\n",
    "# for col in corr_df.columns:\n",
    "#     for row in corr_df.index:\n",
    "#         if corr_df[col].loc[row] != 'nan':\n",
    "#             if col in dic:\n",
    "#                 dic[col].append(row)\n",
    "#             else:\n",
    "#                 dic[col] = [row]\n",
    "\n",
    "# dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch with Cross-Validation\n",
    "To get the best XGB model possible, it is important to try a range of different parameters over the training data. Below is a grid search function that searches every combination between the three parameters provided to it, recording which is the best combination. The metric used for determining the best model is the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dMatrix(X, y=None):\n",
    "    if y is None:\n",
    "        return xgb.DMatrix(X)\n",
    "    else:\n",
    "        return xgb.DMatrix(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_set(X_val, y_val):\n",
    "    return [(xgb.DMatrix(X_val, y_val), 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(hyperParams, train_df, target):\n",
    "    # GridSearch function can only support the exploration of 3 params in hyperParams\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    n_folds = 5\n",
    "    folds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    const_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eta':0.01,\n",
    "        'eval_metric':'rmse',\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    \n",
    "#     Create iterable parameters\n",
    "    grid = ParameterGrid(hyperParams)\n",
    "    for hyper_params in grid:\n",
    "        \n",
    "#         Create dictionary of params \n",
    "        param1, param2, param3 = hyper_params.items()\n",
    "        current_params = {param1[0]: param1[1], param2[0]: param2[1], param3[0]: param3[1]}\n",
    "        \n",
    "        print('##########################################################################\\n')\n",
    "        print(f'Parameters being tested: {current_params}')\n",
    "\n",
    "#         Append param dictionary to model parameters\n",
    "        const_params.update(current_params)\n",
    "        \n",
    "        kfold_gs_score = 0\n",
    "        for train_index, test_index in folds.split(train_df):\n",
    "#             Seperate train/test data\n",
    "            train_X, train_y = train_df.loc[train_index], target[train_index]\n",
    "            test_X, test_y = train_df.loc[test_index], target[test_index]\n",
    "            \n",
    "#             Preparing the training data\n",
    "            eval_set_list = create_eval_set(test_X, test_y)\n",
    "            dMatTrain = to_dMatrix(train_X, train_y)\n",
    "            dMatTest = to_dMatrix(test_X)           \n",
    "            \n",
    "#             Training the model\n",
    "            start = time.time()\n",
    "            gs = xgb.train(\n",
    "                            params=const_params, \n",
    "                            dtrain=dMatTrain, \n",
    "                            num_boost_round=5000, \n",
    "                            evals=eval_set_list,\n",
    "                            early_stopping_rounds=100, \n",
    "                            verbose_eval=1000\n",
    "                            )    \n",
    "            end = time.time()\n",
    "            print(f'Execution time: {np.round((end - start),2)}s\\n')\n",
    "            print('------------------------------------')\n",
    "            \n",
    "            pred_y = np.expm1(gs.predict(dMatTest))\n",
    "        \n",
    "#             Sum kfold model score\n",
    "            kfold_gs_score += np.sqrt(metrics.mean_squared_error(np.expm1(test_y), pred_y))\n",
    "    \n",
    "#         Produce average model score\n",
    "        gs_score = kfold_gs_score / n_folds\n",
    "        print(f'Parameter score: {gs_score}\\n')\n",
    "        \n",
    "#         Only keep the best parameters\n",
    "        if best_score > gs_score:\n",
    "            best_score = gs_score\n",
    "            best_params = current_params\n",
    "    print(f'The best parameters found were: {best_params}')\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_params = {\n",
    "    'subsample': np.arange(0.5,1,0.1).tolist(),\n",
    "    'max_depth': np.arange(3,12,1).tolist(),\n",
    "    'min_child_weight': np.arange(3,6,1).tolist()\n",
    "}\n",
    "print(f'Number of training features: {X.shape[1]} | Number of training rows: {X.shape[0]}')\n",
    "best_params = GridSearch(grid_params, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model\n",
    "Now that we have found our best parameters, we can train our final model and submit to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Eval_set train/test preformance data\n",
    "dX_train = xgb.DMatrix(X_train, y_train)\n",
    "dy_test = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "# Training data\n",
    "dtrain = xgb.DMatrix(X, y)\n",
    "\n",
    "del X_train, X_test, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original algorithm\n",
    "# best_params = {'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8999999999999999}\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eta':0.01,\n",
    "    'eval_metric':'rmse',\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "params.update(best_params)\n",
    "\n",
    "eval_set = [(dX_train, 'train'), (dy_test, 'eval')]\n",
    "bst = xgb.train(\n",
    "                params=params, \n",
    "                dtrain=dtrain, \n",
    "                num_boost_round=5000, \n",
    "                evals=eval_set,\n",
    "                early_stopping_rounds=100, \n",
    "                verbose_eval=1000\n",
    "                )\n",
    "\n",
    "y_pred = np.expm1(bst.predict(dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(path + 'sample_submission.csv')\n",
    "sub['target'] = y_pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
