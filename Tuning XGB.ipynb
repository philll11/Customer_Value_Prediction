{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Load libraries.\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time\nimport gc\n\nimport xgboost as xgb\n\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import datasets.\npath = '../input/santander-value-prediction-challenge/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"ID\", \"target\"], axis=1)\ny = np.log1p(train[\"target\"].values)\n\ntest = test.drop([\"ID\"], axis=1)\n\ndel train\ngc.collect()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"14"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removes features containing constant values\nfeat_to_remove = []\nfor feat in X.columns:\n    if len(X[feat].unique()) == 1:\n        feat_to_remove.append(feat)\n        \nX.drop(feat_to_remove, axis=1, inplace=True)\ntest.drop(feat_to_remove, axis=1, inplace=True)\n\nprint(f'Removed {len(feat_to_remove)} Constant Columns\\n')","execution_count":5,"outputs":[{"output_type":"stream","text":"Removed 256 Constant Columns\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### GridSearch with Cross-Validation\nTo get the best XGB model possible, it is important to try a range of different parameters over the training data. Below is a grid search function that searches every combination between the three parameters provided to it, recording which is the best combination. The metric used for determining the best model is the RMSE."},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_dMatrix(X, y=None):\n    if y is None:\n        return xgb.DMatrix(X)\n    else:\n        return xgb.DMatrix(X, y)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_eval_set(X_val, y_val):\n    return [(xgb.DMatrix(X_val, y_val), 'eval')]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GridSearch(hyperParams, train_df, target):\n    # GridSearch function can only support the exploration of 3 params in hyperParams\n    \n    best_score = np.inf\n    best_params = None\n\n    n_folds = 5\n    folds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n    const_params = {\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'eval_metric':'rmse',\n        'tree_method': 'gpu_hist'\n    }\n    \n#     Create iterable parameters\n    grid = ParameterGrid(hyperParams)\n    for hyper_params in grid:\n        \n#         Create dictionary of params \n        param1, param2, param3 = hyper_params.items()\n        current_params = {param1[0]: param1[1], param2[0]: param2[1], param3[0]: param3[1]}\n        \n        print('##########################################################################\\n')\n        print(f'Parameters being tested: {current_params}')\n\n#         Append param dictionary to model parameters\n        const_params.update(current_params)\n        \n        kfold_gs_score = 0\n        for train_index, test_index in folds.split(train_df):\n#             Seperate train/test data\n            train_X, train_y = train_df.loc[train_index], target[train_index]\n            test_X, test_y = train_df.loc[test_index], target[test_index]\n            \n#             Preparing the training data\n            eval_set_list = create_eval_set(test_X, test_y)\n            dMatTrain = to_dMatrix(train_X, train_y)\n            dMatTest = to_dMatrix(test_X)           \n            \n#             Training the model\n            start = time.time()\n            gs = xgb.train(\n                            params=const_params, \n                            dtrain=dMatTrain, \n                            num_boost_round=5000, \n                            evals=eval_set_list,\n                            early_stopping_rounds=100, \n                            verbose_eval=1000\n                            )    \n            end = time.time()\n            print(f'Execution time: {np.round((end - start),2)}s\\n')\n            print('------------------------------------')\n            \n            pred_y = np.expm1(gs.predict(dMatTest))\n        \n#             Sum kfold model score\n            kfold_gs_score += np.sqrt(metrics.mean_squared_error(np.expm1(test_y), pred_y))\n    \n#         Produce average model score\n        gs_score = kfold_gs_score / n_folds\n        print(f'Parameter score: {gs_score}\\n')\n        \n#         Only keep the best parameters\n        if best_score > gs_score:\n            best_score = gs_score\n            best_params = current_params\n    print(f'The best parameters found were: {best_params}')\n    return best_params","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\ngrid_params = {\n    'subsample': np.arange(0.5,1,0.1).tolist(),\n    'max_depth': np.arange(3,12,1).tolist(),\n    'min_child_weight': np.arange(3,6,1).tolist()\n}\nprint(f'Number of training features: {X.shape[1]} | Number of training rows: {X.shape[0]}')\nbest_params = GridSearch(grid_params, X, y)","execution_count":35,"outputs":[{"output_type":"stream","text":"Number of training features: 4735 | Number of training rows: 4459\n##########################################################################\n\nParameters being tested: {'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.5}\n[0]\teval-rmse:13.9511\nWill train until eval-rmse hasn't improved in 100 rounds.\n[1000]\teval-rmse:1.4714\nStopping. Best iteration:\n[1506]\teval-rmse:1.46148\n\nExecution time: 10.13s\n\n------------------------------------\n[0]\teval-rmse:13.9247\nWill train until eval-rmse hasn't improved in 100 rounds.\n[1000]\teval-rmse:1.52323\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-34-f7a8f2fd5b17>\u001b[0m in \u001b[0;36mGridSearch\u001b[0;34m(hyperParams, train_df, target)\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                             )    \n\u001b[1;32m     56\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Final Model\nNow that we have found our best parameters, we can train our final model and submit to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n# Eval_set train/test preformance data\ndX_train = xgb.DMatrix(X_train, y_train)\ndy_test = xgb.DMatrix(X_test, y_test)\n\n# Training data\ndtrain = xgb.DMatrix(X, y)\n\n# del X_train, X_test, y_train, y_test\ngc.collect()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtest = xgb.DMatrix(test)\ndel test","execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'test' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-58d9dabfc916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# original algorithm\nbest_params = {'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8999999999999999}\nparams = {\n    'objective': 'reg:squarederror',\n    'eta':0.01,\n    'eval_metric':'rmse',\n    'tree_method': 'gpu_hist'\n}\n\n# params.update(best_params)\n\neval_set = [(dX_train, 'train'), (dy_test, 'eval')]\nbst = xgb.train(\n                params=params, \n                dtrain=dtrain, \n                num_boost_round=5000, \n                evals=eval_set,\n                early_stopping_rounds=100, \n                verbose_eval=1000\n                )","execution_count":18,"outputs":[{"output_type":"stream","text":"[0]\ttrain-rmse:13.9626\teval-rmse:13.9538\nMultiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n\nWill train until eval-rmse hasn't improved in 100 rounds.\n[1000]\ttrain-rmse:1.12053\teval-rmse:1.10625\n[2000]\ttrain-rmse:0.993619\teval-rmse:0.985007\n[3000]\ttrain-rmse:0.908328\teval-rmse:0.900145\n[4000]\ttrain-rmse:0.84454\teval-rmse:0.835424\n[4999]\ttrain-rmse:0.794802\teval-rmse:0.78424\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.expm1(bst.predict(dtest))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(path + 'sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}